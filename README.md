# Dimensionality-Reduction-from-Scratch-Feature-Selection-PCA-LDA

## Project Overview ##
This project is an in-depth exploration of dimensionality reduction techniques applied to the Iris dataset. It demonstrates the implementation of forward selection, Principal Component Analysis (PCA), and Linear Discriminant Analysis (LDA) from scratch. The goal is to illustrate how these methods can significantly reduce the feature space while preserving crucial information for model accuracy.

## Techniques Implemented ##
#### Forward Selection: #### A sequential feature selection method that begins with an empty model and adds features iteratively, choosing those that most improve the model at each step.
#### Principal Component Analysis (PCA): #### A statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.
### Linear Discriminant Analysis (LDA): #### A method that seeks to find a linear combination of features that best separate two or more classes, aiming to maximize the ratio of between-class variance to within-class variance.

## Dataset ##
The project uses the Iris dataset, renowned in pattern recognition literature. It comprises three classes of 50 samples each, with each class referring to a type of iris plant and several measurements for each sample.

## Requirements ##
To run this project, you'll need Python 3.x and the following libraries:

NumPy
Matplotlib
Scikit-learn

For additional insights into dimensionality reduction and its practical applications and also what the output implies, especially in dealing with high-dimensional datasets, consider reading my blog post https://medium.com/@padmanabhbutala03/the-curse-of-dimensionality-navigating-through-high-dimensional-spaces-b7044b6c07b5. 
